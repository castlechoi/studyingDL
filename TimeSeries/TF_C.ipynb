{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYc3E0jrbNNZZxc9aeU2XR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/castlechoi/studyingDL/blob/main/TimeSeries/TF_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NuJ0BM9ErRI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Config():\n",
        "  def __init__(self):\n",
        "    # data feature\n",
        "    self.target_num_classes = 2\n",
        "    self.feature_len = 176\n",
        "\n",
        "    self.subset = True\n",
        "\n",
        "    # optimizer parameter\n",
        "    self.batch_size = 64\n",
        "    self.num_epochs = 40\n",
        "\n",
        "    # augmentation parameter\n",
        "    self.sigma"
      ],
      "metadata": {
        "id": "W7CkLQptYtdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super(ResnetBackbone, self).__init__()\n",
        "\n",
        "    self.layer1 = nn.Sequential(\n",
        "      nn.Conv1d(1, 32, kernel_size = 8, stride = 8),\n",
        "      nn.BatchNorm1d(),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool1d(kernel_size = 2, stride = 2)\n",
        "    )\n",
        "    self.layer2 = nn.Sequential(\n",
        "        nn.Conv1d(32, 64, kernel_size = 8, stride = 1),\n",
        "        nn.BatchNorm1d(),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool1d(kernel_size = 2, stride = 2)\n",
        "    )\n",
        "    self.layer3 = nn.Sequential(\n",
        "        nn.Conv1d(64,128, kernel_size = 8, stride = 1),\n",
        "        nn.BatchNorm1d(),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "    assert config.feature_len % 8 == 0\n",
        "    self.feature_len = config.feature_len / 8 - 14\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(self.feature_len,256),\n",
        "        nn.BatchNorm1d(256)\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256.128),\n",
        "        )\n",
        "\n",
        "  def forward(self, x): \n",
        "    #  batch_size * channel * feature_len \n",
        "    #  64 * 1 * 200\n",
        "    out = self.layer1(x)    # 64 * 32 * 25\n",
        "    out = self.layer2(out)  # 64 * 64 * (18)\n",
        "    out = self.layer3(out)  # 64 * 128 * (8)\n",
        "    out = out.view(out.size(0),-1)  # flatten\n",
        "    out = self.fc(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "I0NJAjfKKV3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TFC(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super(TFC, self).__init__()\n",
        "    self.encoder_t = Encoder(config)\n",
        "    self.encoder_f = Encoder(config)\n",
        "\n",
        "    self.projector_t = nn.Sequential(\n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128)\n",
        "            )\n",
        "    self.projector_f = nn.Sequential(\n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128)\n",
        "            )\n",
        "\n",
        "  def forward(self,x_in_t, x_in_f):\n",
        "    h_t = self.encoder_t(x_in_t)\n",
        "    z_t = self.projector_t(h_t)\n",
        "\n",
        "    h_f = self.encoder_f(x_in_f)\n",
        "    z_f = self.projector_f(h_f)\n",
        "\n",
        "    return h_t, z_t, h_f, z_f"
      ],
      "metadata": {
        "id": "pwzLCGsQHY2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TaskClassifier(nn.module):\n",
        "  def __init__(self,config):\n",
        "    super(TaskClassifier, self).__init__()\n",
        "    self.fc1 = nn.Linear(256,64)\n",
        "    self.fc2 = nn.Linear(64,config.target_num_classes)\n",
        "\n",
        "    self.sigmoid = nn.Softmax(dim = 1)\n",
        "  \n",
        "  def forward(self, x_t, x_f):\n",
        "    out = self.fc1(x)\n",
        "    # flatten\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.sigmoid(out)\n",
        "    out = self.fc2(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "oGTv75CdYRtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NTXentLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NTXentLoss, self).__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "    return x"
      ],
      "metadata": {
        "id": "Fsg0onH3k1SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataAugmentation(self, data, config, domain = \"time\"):\n",
        "  if domain == \"time\":\n",
        "    # jittering\n",
        "    return x + np.random_normal(loc = 0, scale = config.sigma, size = data.shape)\n",
        "  else:\n",
        "    # remove freq\n",
        "    mask = torch.cuda.FloatTensor(data.shape).uniform_() > 0\n",
        "    mask = mask.to(data.device)\n",
        "    return data * mask"
      ],
      "metadata": {
        "id": "Z2kkRHKOZoEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LoadDataset(Dataset):\n",
        "  def __init__(self, dataset, config, training_mode):\n",
        "    super(LoadDataset, self).__init__()\n",
        "\n",
        "    self.x_train = dataset[\"samples\"]\n",
        "    self.y_train = dataset[\"labels\"]\n",
        "\n",
        "    # shuffle the data\n",
        "    data = list(zip(x_train, y_train))\n",
        "    np.random.shuffle(data)\n",
        "    x_train, y_train = zip(*data)\n",
        "    # np -> torch\n",
        "    x_train, y_train = torch.stack(list(x_train), dim = 0), torch.stack(list(y_train), dim = 0)\n",
        "\n",
        "    if config.subset == True:\n",
        "      subset_size = 64 * 10\n",
        "      x_train = x_train[:subset_size]\n",
        "      y_train = y_train[:subset_size]\n",
        "\n",
        "    self.x_data = x_train [:,:1,:176] # Epilepsy length 178\n",
        "    self.y_data = y_train\n",
        "\n",
        "    self.x_data_f = ff.fft(self.x_data).abs()\n",
        "    self.len = x_train.shape[0]\n",
        "\n",
        "    if training_mode == \"pre_train\":\n",
        "      self.aug1 = dataAugmentation(self.x_data, config,\"time\")\n",
        "      self.aug1_f = dataAugmentation(self.x_data_f, config,\"freq\")\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    if self.training_mode == \"pre_train\":\n",
        "      return self.x_data[index], self.aug1[index], self.x_data_f[index], self.aug1_f[index], self.y_data[index]\n",
        "    else:\n",
        "      return self.x_data[index], self.x_data[index], self.x_data_f[index], self.x_data_f[index], self.y_data[index]\n",
        "  def __len__(self):\n",
        "    return self.len"
      ],
      "metadata": {
        "id": "nBYBTaXnXBjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(config):\n",
        "  pretrain_dataset = torch.load(os.path.join('./SleepEEG', \"train.pt\"))\n",
        "  fintune_train_dataset = torch.load(os.path.join('./Epilepsy',\"train.pt\"))\n",
        "  finetune_valid_dataset = torch.load(os.path.join('./Epilepsy',\"valid.pt\"))\n",
        "  finetune_test_dataset = torch.load(os.path.join('./Epilepsy',\"test.pt\"))\n",
        "  \n",
        "  pretrain_dataset = LoadDataset(pretrain_dataset, config, \"pre_train\")\n",
        "  finetune_train_dataset = LoadDataset(fintune_train_dataset, config, \"finetune\")\n",
        "  finetune_valid_dataset = LoadDataset(finetune_valid_dataset, config, \"finetune\")\n",
        "  finetune_test_dataset = LoadDataset(finetune_test_dataset, config, \"finetune\")\n",
        "\n",
        "  pret_loader = torch.utils.data.DataLoader(dataset=pretrain_dataset, batch_size=64, shuffle=True, drop_last=True,num_workers=0)\n",
        "  fine_train_loader = torch.utils.data.DataLoader(dataset=finetune_train_dataset, batch_size=64, shuffle=True, drop_last=True,num_workers=0)\n",
        "  fine_valid_loader = torch.utils.data.DataLoader(dataset=finetune_valid_dataset, batch_size=64, shuffle=True, drop_last=True,num_workers=0)\n",
        "  fine_test_loader = torch.utils.data.DataLoader(dataset=finetune_test_dataset, batch_size=64, shuffle=True, drop_last=True,num_workers=0)\n",
        "\n",
        "  return pret_loader,fine_train_loader,fine_valid_loader,fine_test_loader"
      ],
      "metadata": {
        "id": "Nlc-fjcsanuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main\n",
        "config = Config()\n",
        "\n",
        "model = TFC(config)\n",
        "classifier = TaskClassifier(config)\n",
        "\n",
        "model_optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4, weight_decay = 5e-4)\n",
        "classifier_optimizer = torch.optim.Adam(classifier.parameters(), lr = 3e-4, weight_decay = 5e-4)\n",
        "\n",
        "pret_criterion = NTXentLoss()\n",
        "classification_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# load data\n",
        "pret_dl, fine_train_dl, fine_val_dl, fine_test_dl = load_dataset(config)"
      ],
      "metadata": {
        "id": "rKw8WqR4YNO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre-Train\n",
        "for epoch in range(config.num_epochs):\n",
        "  pret_loss = 0\n",
        "  for x_t, x_t_aug, x_f, x_f_aug, _ in pret_dl:\n",
        "    model_optimizer.zero_grad()\n",
        "    h_t, z_t, h_f, z_f =  model(x_t, x_f)\n",
        "    h_t_aug, z_t_aug, h_f_aug, z_f_aug = model(x_t_aug, x_f_aug)\n",
        "\n",
        "    loss_t = pret_criterion(h_t, h_t_aug)\n",
        "    loss_f = pret_criterion(h_f, h_f_aug)\n",
        "    # positive pair in TF embedding space\n",
        "    loss_c_p = pret_criterion(z_t, z_f)\n",
        "    # negative pairs in TF embedding space\n",
        "    loss_c_n1 = pret_criterion(z_t, z_f_aug)\n",
        "    loss_c_n2 = pret_criterion(z_f, z_t_aug)\n",
        "    loss_c_n3 = pret_criterion(z_t_aug, z_f_aug)\n",
        "\n",
        "    loss_c = (loss_c_p - loss_c_n1 + 1) + (loss_c_p - loss_c_n2 + 1) + (loss_c_p - loss_c_n3 + 1)\n",
        "\n",
        "    lam = 0.5\n",
        "    loss_tfc = lam * (loss_t + loss_f) + (1-lam) * loss_c\n",
        "\n",
        "    loss_tfc.backward()\n",
        "    model_optimizer.step()\n",
        "    pret_loss += loss_tfc.item()\n",
        "\n",
        "  print(f'Epoch {epoch+1} Loss : {pret_loss / epoch} ')\n",
        "\n",
        "# FineTuning\n",
        "valid_loss_list = []\n",
        "global best_model, best_classifier\n",
        "for epoch in range(config.num_epochs):\n",
        "  finetune_loss = 0\n",
        "  for x_t, _, x_f, _, label in fine_train_dl:\n",
        "    model_optimizer.zero_grad()\n",
        "    classifier_optimizer.zero_grad()\n",
        "    h_t, z_t, h_f, z_f =  model(x_t, x_f)\n",
        "    pred_class = classifier(z_t,z_f)\n",
        "\n",
        "    loss_t = pret_criterion(h_t, h_t_aug)\n",
        "    loss_f = pret_criterion(h_f, h_f_aug)\n",
        "    # positive pair in TF embedding space\n",
        "    loss_c_p = pret_criterion(z_t, z_f)\n",
        "    # negative pairs in TF embedding space\n",
        "    loss_c_n1 = pret_criterion(z_t, z_f_aug)\n",
        "    loss_c_n2 = pret_criterion(z_f, z_t_aug)\n",
        "    loss_c_n3 = pret_criterion(z_t_aug, z_f_aug)\n",
        "\n",
        "    loss_c = (loss_c_p - loss_c_n1 + 1) + (loss_c_p - loss_c_n2 + 1) + (loss_c_p - loss_c_n3 + 1)\n",
        "\n",
        "    lam = 0.5\n",
        "    loss_tfc = lam * (loss_t + loss_f) + (1-lam) * loss_c\n",
        "    loss_p = classification_criterion(pred_class, label)\n",
        "\n",
        "    loss = loss_p + loss_tfc\n",
        "\n",
        "    loss.backward()\n",
        "    model_optimizer.step()\n",
        "    classifier_optimizer.zero_grad()\n",
        "    finetune_loss += loss.item()\n",
        "  \n",
        "  print(f'Finetune Epoch {epoch+1} Loss : {finetune_loss / epoch} ')\n",
        "  # valid\n",
        "  valid_loss = 0\n",
        "  test_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for x_t, _, x_f, _, label in fine_valid_dl:\n",
        "      h_t, z_t, h_f, z_f =  model(x_t, x_f)\n",
        "      pred_class = classifier(z_t,z_f)\n",
        "\n",
        "      loss_p = classification_criterion(pred_class, label)\n",
        "\n",
        "      valid_loss += loss_p.item()\n",
        "    print(f'Valid Epoch {epoch+1} Loss : {valid_loss / epoch} ')\n",
        "  valid_loss_list.append(valid_loss)\n",
        "\n",
        "  if(min(valid_loss_list) > valid_loss) or epoch == 0:\n",
        "    best_model = model.state_dict()\n",
        "    best_classifier = classifier.state_dict()\n",
        "  else:\n",
        "    model.load_state_dict(best_model)\n",
        "    classifier.load_state_dict(best_classifier)\n",
        "    \n",
        "  # test\n",
        "  with torch.no_grad():\n",
        "    for x_t, _, x_f, _, label in fine_test_dl:\n",
        "      h_t, z_t, h_f, z_f =  model(x_t, x_f)\n",
        "      pred_class = classifier(z_t,z_f)\n",
        "\n",
        "      loss_p = classification_criterion(pred_class, label)\n",
        "\n",
        "      valid_loss += loss_p.item()\n",
        "    print(f'Valid Epoch {epoch+1} Loss : {valid_loss / epoch} ')"
      ],
      "metadata": {
        "id": "6db8JxGudkNj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}