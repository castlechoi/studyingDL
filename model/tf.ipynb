{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgI8zcmS2Tey7Jr4eMY0S5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### nn.TransformerEncoderLayer\n",
        "nn.TransformerEncoderLayer(\n",
        "  d_model : (int)  input의 features 수\n",
        "  n_head  : (int)  multi-attention의 head 수\n",
        "  dim_feedforward : default = 2048\n",
        "  dropout : default 0.1\n",
        "  activation : relu or gelu\n",
        "  batch_first\n",
        "  layer_norm_eps : layer norm할 때 사용\n",
        "  norm_first\n",
        ")\n",
        ".forward(\n",
        "  src : encoderlayer의 sequence\n",
        "  src_mask : src_sequence의 mask\n",
        "  srk_key_padding_mask : the mask for the src keys per batch\n",
        ")\n",
        "\n",
        "### nn.TransformerEncoder\n",
        "nn.TransformerEncoder(\n",
        "\n",
        "  encoder_layer : TransformerEncoderLayer 클래스\n",
        "  num_layers : sub-encoder layer의 갯수\n",
        "  norm : layer 정규화 요소\n",
        "  enable_nested_tesnor : nested tensor로 바꿔줌 ->  padding이 많으면 성능 개선\n",
        "\n",
        "### nn.TransformerDecoder\n",
        "  decoder layers의 stack"
      ],
      "metadata": {
        "id": "62fd3GcN6OQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "6UA7IAm73n0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er7UKqdK3CYR"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, n_token, d_model, n_head, d_hid, n_layer):\n",
        "    super(Transformer, self).__init()\n",
        "\n",
        "    self.pos_encoder = PositionalEncoding(d_model, dropout = 0.3)\n",
        "    encoder_layers = TransformerEncoderLayer(d_model, n_head, d_hid, dropout = 0.3)\n",
        "    self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "    self.encoder = nn.Embedding(n_token, d_model)\n",
        "    self.d_model = d_model\n",
        "    self.decoder = nn.Linear(d_model, ntoken)\n",
        "\n",
        "    self.init_weights()\n",
        "  \n",
        "  def init_weights(self):\n",
        "    initrange = 0.1\n",
        "    self.encoder.weight.data.uniform_(-initrange, initrange)"
      ]
    }
  ]
}