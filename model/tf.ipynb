{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGzWaGHrh9NVm1gqGxw71p"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "7BWeBJlvLjI2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('./total_data.csv', encoding = 'cp949')\n",
        "df.head()\n",
        "df = df[df['종목명'] == 'KB금융']"
      ],
      "metadata": {
        "id": "NwjVHI2zLr0N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_np = df['종가'].values\n",
        "print(len(df_np))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrYMIp2bMUWY",
        "outputId": "196f4794-7695-4126-973a-f5aab97b6f8e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data split\n",
        "# input_dim = 50\n",
        "# forecast_dim = 3\n",
        "\n",
        "\"\"\"\n",
        "  N-BEATS에서 window_size와 horizon size를 n배수에 설정했는데\n",
        "  이를 적용했더니 train_loss, test_loss 모두 감소소\n",
        "\"\"\"\n",
        "window_size = 50\n",
        "horizon_size = 5\n"
      ],
      "metadata": {
        "id": "HAZlN3hySAuJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "train_x = []\n",
        "train_y = []\n",
        "\n",
        "mm = MinMaxScaler()\n",
        "fitted = mm.fit(df_np.reshape(-1,1))\n",
        "out = mm.transform(df_np.reshape(-1,1))\n",
        "df_np = out.reshape(-1)\n",
        "for i in range(len(df_np) - window_size - horizon_size):\n",
        "  train_x.append(df_np[i:i+window_size]) # len == window_size\n",
        "  train_y.append(df_np[i+window_size: i+window_size + horizon_size]) # len == horizon_size\n",
        "\n",
        "x_tensor = torch.FloatTensor(train_x)\n",
        "y_tensor = torch.FloatTensor(train_y)\n",
        "print(f'src shape : {x_tensor.shape}')\n",
        "print(f'label shape : {y_tensor.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Lddu-7DSuoH",
        "outputId": "0a40d7ad-277a-42a5-b79b-711de8b05502"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src shape : torch.Size([932, 50])\n",
            "label shape : torch.Size([932, 5])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-c6e031b86d00>:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  x_tensor = torch.FloatTensor(train_x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "QM-1FMeYjgKE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,d_model, n_head, num_enc):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    # encoder layer parameter\n",
        "    self.d_model = d_model\n",
        "    self.n_head = n_head\n",
        "    self.num_enc = num_enc\n",
        "\n",
        "    self.pos_encoder = PositionalEncoding(d_model, 0.1)\n",
        "\n",
        "    self.encoderBlock = nn.TransformerEncoderLayer(\n",
        "      d_model = self.d_model,\n",
        "      nhead = self.n_head,\n",
        "      batch_first = True\n",
        "    )\n",
        "\n",
        "    self.encoder = nn.TransformerEncoder(\n",
        "        encoder_layer = self.encoderBlock,\n",
        "        num_layers = self.num_enc\n",
        "    )\n",
        "    \n",
        "    self.decoder = nn.Linear(d_model, d_model//2)\n",
        "    self.fc = nn.Linear(d_model//2, 5)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "      # encoder\n",
        "    # out = self.pos_encoder(x)\n",
        "    # out = self.encoder(out.transpose(0,1))\n",
        "    out = self.encoder(x)\n",
        "    \n",
        "      # encoder output == decoder input\n",
        "      # encoder output shape = encoder input shape\n",
        "      # decoder\n",
        "    out = self.decoder(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.fc(out)\n",
        "\n",
        "      # forecast horizon which has a length of 3\n",
        "    return out"
      ],
      "metadata": {
        "id": "uHwcZV7mMkWQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N-Beats loss\n",
        "\n",
        "\"\"\"\n",
        "  sMAPE Loss is not affected by unscaled feature\n",
        "\"\"\"\n",
        "class sMAPE(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(sMAPE, self).__init__()\n",
        "  \n",
        "  def forward(self, src,tgt):\n",
        "    # src shape =  (700,1,5)\n",
        "    # tgt shape = (700,1,5)\n",
        "    tot = 0\n",
        "    s = src.view(-1,5)\n",
        "    t = tgt.view(-1,5)\n",
        "\n",
        "    up = torch.abs(s - t) # shape ( 700 , 3)\n",
        "    down =torch.abs(s) + torch.abs(t) # (700,3)\n",
        "\n",
        "    tot = torch.mean( up / down, -1) * 200 # (700)\n",
        "    return torch.mean(tot)"
      ],
      "metadata": {
        "id": "fRmMuateaboN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "3lZ0aKnhWG_F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_tensor = x_tensor.view(-1,1,window_size).to(device)\n",
        "y_tensor = y_tensor.view(-1,1,horizon_size).to(device)\n",
        "print(f'src shape : {x_tensor.shape}')\n",
        "print(f'label shape : {y_tensor.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMX6hMLNT8JH",
        "outputId": "06a8a3f7-bc1c-4226-c22e-605fa4262394"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src shape : torch.Size([932, 1, 50])\n",
            "label shape : torch.Size([932, 1, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# learning hyperparameters\n",
        "lr = 0.001\n",
        "num_epochs = 600"
      ],
      "metadata": {
        "id": "hIhYNnhwzwir"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model init\n",
        "model = Transformer(window_size,2,2).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "criterion = sMAPE()"
      ],
      "metadata": {
        "id": "MS2uZbsOR_ZO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_test_split\n",
        "train_x = x_tensor[:700]\n",
        "train_y = y_tensor[:700]\n",
        "test_x = x_tensor[700:]\n",
        "test_y = y_tensor[700:]"
      ],
      "metadata": {
        "id": "t2YLU_IkUscf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs+1):\n",
        "  optimizer.zero_grad()\n",
        "  pred = model(train_x)\n",
        "  loss = criterion(pred, train_y)\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "  if epoch % 100 == 0:\n",
        "    print(f'Epoch : {epoch} , Loss : {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBQrRairUjV3",
        "outputId": "c39e19f2-0e6f-4ae4-c311-f900d2cab2d1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0 , Loss : 172.16566467285156\n",
            "Epoch : 100 , Loss : 17.021360397338867\n",
            "Epoch : 200 , Loss : 10.222859382629395\n",
            "Epoch : 300 , Loss : 8.499074935913086\n",
            "Epoch : 400 , Loss : 7.859388828277588\n",
            "Epoch : 500 , Loss : 6.958000183105469\n",
            "Epoch : 600 , Loss : 8.180373191833496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "y_pred = model(test_x)\n",
        "tot_loss = criterion(y_pred, test_y)\n",
        "print(f'Loss : {tot_loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kFxk-q8W3Pd",
        "outputId": "7e489d10-fbd6-4eb0-d280-fcd4696964ea"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss : 8.4104\n"
          ]
        }
      ]
    }
  ]
}